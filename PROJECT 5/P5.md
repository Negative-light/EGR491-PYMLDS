# Project 5: Making an AI

## 1. Table of Display

|                                Topic | Method         |
| -----------------------------------: | :------------- |
|                           What is ML | Summary        |
|                Intro to Scikit-Learn | Summary w/Code |
| Hyperparameters and Model Validation | ?              |
|                  Feature Engineering | ?              |
|            DTrees and Random Forests | ?              |
|              Face Detection Pipeline | Final Project  |

## 2. Table Of Contents

1. [1. Table of Display](#1-table-of-display)
2. [2. Table Of Contents](#2-table-of-contents)
3. [3. Background](#3-background)
   1. [3.1. What is Machine Learning?](#31-what-is-machine-learning)
      1. [3.1.1. Supervised learning](#311-supervised-learning)
         1. [3.1.1.1. Classification](#3111-classification)
         2. [3.1.1.2. Regression](#3112-regression)
      2. [3.1.2. Unspervised Learning](#312-unspervised-learning)
         1. [3.1.2.1. Clustering](#3121-clustering)
         2. [3.1.2.2. Dimensionality Reduction](#3122-dimensionality-reduction)
   2. [3.2. Scikit-Learn](#32-scikit-learn)
      1. [3.2.1. Data Structure](#321-data-structure)
      2. [3.2.2. Imputation: Filling Missing Data](#322-imputation-filling-missing-data)
   3. [Feature Engineering: What are we working with?](#feature-engineering-what-are-we-working-with)
      1. [Categorical Data](#categorical-data)
      2. [Language Features](#language-features)
   4. [Picking a Model: What will you use?](#picking-a-model-what-will-you-use)
   5. [3.3. Model Validation: Will this Model Work](#33-model-validation-will-this-model-work)
   6. [3.4. Special Topic: Random Forests](#34-special-topic-random-forests)
4. [4. Project: Face Detection Pipeline](#4-project-face-detection-pipeline)
   1. [4.1. Output](#41-output)
   2. [4.2. Code](#42-code)

## 3. Background

### 3.1. What is Machine Learning?

Machine Learning (ML) is the art of designing mathematical models of data that can be taught via tunable parameters. Because these algorithms are designed to asist in understanding data, there can be some debate of whether ML could be considered a branch of Artificial Intelligence (AI) anymore.

Because ML works with big data which vary greatly in both size, complexity there must also be various methods of analizing this data. The two general categories of ML are **Supervised** and **Unsupervised** learning. We will explorw these methods futher in this paper.

#### 3.1.1. Supervised learning

Supervised learning takes data and labels associated with the data to model their relationship. This ML model is used to apply labels to novel data. Supervised learning is commonly subdivided into **classification** and **regression**.

##### 3.1.1.1. Classification

Classification models use descrite categories such as a status. This type of model may be used to identify objects in an image or seek for when a part might be damaged within a piece of machinery.

This model will require the devleoper to fist make a labeled dataset. Then you must design model inputs and the general assumptions your can provide. After this you can provide a set of model paramters which can be adjusted by the model during the training stage. The end requlst is that when introduced to novel data the classical model will provide a predictive label.

##### 3.1.1.2. Regression

Regression models place their categories in a continuous spectrums. This can be used to seek the pobablity of damage to an piece of machinery or seek more complex relationships such as genetics to regions.

Regression models could be treadted as the oposite of [Demensionality Reduction](#3122-dimensionality-reduction) models. They will extract a new unknown relationship and create a new dimesion of labels.

#### 3.1.2. Unspervised Learning

Unsuprvised learning results from not having a labeled dataset. This type of learning is used to find relationships within a data set. Unspervised Learning can be further subdivided into **Clustering** and **Demensitionaly Reduction** models.

##### 3.1.2.1. Clustering

Clustering models act simlarly to classification models exceept they are seeking the distinct groups with the dataset. This can be used to seekout groups within massive datasets that humans may never see.

One method to do clustersing is through the _k_-means fits model. This method finds the center of data clusters, it must be profivided with a value _k_ or this value may be tunable. This model seeks the postion of centers that has the minimum distance between all points in the dataset.

##### 3.1.2.2. Dimensionality Reduction

Demensionality Reduction as its name suggests is designed to simpfly a data set into the smalled scturcure possible. This type of model could be used to seek important parameters to watch in larger systems which may take years for a human to analyze. This method allows us to infer new strutures that may not be labels (or may not exist).

A demnstionality reduction model typcially will remove one or more of the layers of data. This could be used to review a large dataset and place the data neatly into graphs that huamns and easily understand, interpret and apply.

### 3.2. Scikit-Learn

While developing a ML algrithem from scratch is possible it could be cumbersom and likly has already been done. Thus to prevent reinventing the wheel, you can use the modeule **Scikit-Learn**. Scikit-Learn contains many of the popular algorithems in one consitent API.

Designed by a group of Cornell University students, Scikit-Learn is intended to be a new user friendly API which can also be useful for experts in the field. To this end Scikit-Learn features <ins>consistent</ins> usage, <ins>inspecettable</ins> attributes, <ins> simplistic</ins> set of classes, and <ins> senible defaults </ins> for all classes.

#### 3.2.1. Data Structure

Scikit-Learn descibes data with a _features matrix_ this matrix can be stored in a NumPy array or Pandas DataFrame. The rows of the features matrix are often called sameles and the columns are considered features. Thus the features matrix is considered to have the shape `[n_samples, n_features]`.

The features matrix is then coupled with a _label array_. This array is typically is a Numpy array or Pandas Series of n_samples, but it can be two-dimesional with the shapre [n_smaple, n_targets]. The label array could be considered the dependent variable the model is attempting to predict.

#### 3.2.2. Imputation: Filling Missing Data

In an ideal world we would have all the data avalible to us perfectly setup and ready to be used. This is rarly the case. To make filling in data smoother sklearn includes the _Imputer_ class. This class uses various stregies to fill you datasheets.

To use the Imputer class use:

---

```python
from sklearn.impute import SimpleImputer
```

---

Then you can assign the imputer to a variable and use the `.giy_transform()` function. As seen in the example below

```python
### IMPUTATION HOW TO ###
# %%
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from numpy import nan
from sklearn.impute import SimpleImputer
from sklearn.impute import KNNImputer

data_main = np.array([[nan, 0,   3],
                      [3,   7,   9],
                      [nan,   5,   nan],
                      [4,   nan, 6],
                      [nan,   8,   1],
                      [1, 1, 1],
                      [3, 4, 9]])
imp = SimpleImputer(strategy='mean')
data_mean = imp.fit_transform(data_main)
imp = SimpleImputer(strategy='median')
data_median = imp.fit_transform(data_main)
imp = SimpleImputer(strategy='most_frequent')
data_freq = imp.fit_transform(data_main)
imp = SimpleImputer(strategy='constant', fill_value=2)
data_const = imp.fit_transform(data_main)
imp = KNNImputer(n_neighbors=3)
data_KNN = imp.fit_transform(data_main)
# %%
print(data_main)
print('--------------')
print(data_mean)
print('--------------')
print(data_median)
print('--------------')
print(data_freq)
print('--------------')
print(data_const)
print('--------------')
print(data_KNN)

# %%
sns.set()
plt.scatter(data_main[:, 0], data_main[:, 1],
            marker='v', label='Actual', s=200)
plt.scatter(data_mean[:, 0], data_mean[:, 1],
            alpha=0.75, label='Mean', marker='^')
plt.scatter(data_median[:, 0], data_median[:, 1],
            alpha=0.75, label='Median', marker='s')
plt.scatter(data_freq[:, 0], data_freq[:, 1],
            alpha=0.75, label='Frequent', marker='d')
plt.scatter(data_main[:, 0], data_main[:, 1], marker='.',
            label='Const', edgecolors='red', facecolors='none', s=200)
plt.scatter(data_KNN[:, 0], data_KNN[:, 1],
            marker='p', label='KNN')

plt.legend()

plt.savefig('..\images\IMPUTATION.jpg')
```

![OUTPUT](images/IMPUTATION.jpg)

There are four methods that you can use with a normal imputer.

- Mean: Fills the mean of the column within a numerical dataset
- Median: Fills the median of the column within a numerical dataset
- Most Frequent: Fills the most freqent value within the dataset.
- Constant: Fills a constatnt provided in `fill_value`

There is also a newer imputer you may have notied in the code. This is the KNNImputer or k_nearest neighbors imputer. This imputer uses the k-Nearest Neighbors algorithm to estiamte a value. You can set the number of neighbors using the `n_neighbors` parameter.

Their is a experamental imputer known as the IterativeImputer which you can find in the [scikit-learn API](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html#sklearn.impute.IterativeImputer).

### Feature Engineering: What are we working with?

Another import part of preparing data for your model requires building features. This is becuase data provided rarly has a clear division of features to sample making it hard to design the features matrix. The art of filling out the features of a dataset is called **feature engineering**.

First we must acknolege a few types of features:

- _Numerical features_: These featrures are algebraic in nature and are inhernt to the data
- _Categoical Data_: This data deals with categories of the data such as the if a person is tall or short
- Text or Lange Features: These features are considered with the words within a dataset. This particualar feature is common in social media and is the bane of youtubers.
- Image Feature: These features are specific to images such as position and color.

#### Categorical Data

Sklearn does have a few useful features when conducting _vectorization_. When working with categorical data you can use a **DictVecorizer**. This will allow you to seek the features rather quickly.

```python
# %%
from sklearn.feature_extraction import DictVectorizer
data = [{'power': 100, 'usage': 31, 'type': 'development'},
        {'power': 5, 'usage': 23, 'type': 'development'},
        {'power': 35, 'v': 37, 'type': 'gameing'}]

v = DictVectorizer(sparse=False)
X = v.fit_transform(data)
print('----------------')
print(X)
print('----------------')
print(v.inverse_transform(X))
print('----------------')
print(v.transform({'power': 33, 'usage': 21, 'type': 'gameing'}))

# %%
```

OUTPUT

---

      from sklearn.feature_extraction import DictVectorizer...
      ----------------
      [[100.   1.   0.  31.]
      [  5.   1.   0.  23.]
      [ 35.   0.   1.  37.]]
      ----------------
      [{'power': 100.0, 'type=development': 1.0, 'usage': 31.0}, {'power': 5.0, 'type=development': 1.0, 'usage': 23.0}, {'power': 35.0, 'type=gameing': 1.0, 'usage': 37.0}]
      ----------------
      [[33.  0.  1. 21.]]

---

You can retrive a DictVectorizer's feature names through the function `.get_feature_names()`. If we run that on our example we get
['power', 'type=development', 'type=gameing', 'usage']

#### Language Features

Language Features focus on word counts or how many times a specific word occurs within a dataset. ALthough you could do this by hand sklearn provided the effiecent class called **CountVecorizer**. This can take a sequence of items, or text file to tokenize the strings.

As an example we will use the descriptions of these youtube videos [EGR491-SAMPLE_DATA](https://youtube.com/playlist?list=PLZK8Egerqxghhe0xH6_kLpGWg2x8zTgXi).

```python
# %%
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
f = open(
    '..\CODE\INPUT\COUNTVEC_TEST.txt', 'r')
content = f.read()
content = str.split(content, '>')
cv = CountVectorizer()
fit = cv.fit_transform(content)
wordBYword = pd.DataFrame(fit.toarray(), columns=cv.get_feature_names())
# %% VECTORIZE BY 2-5 WORD SEQUENCE
cv2 = CountVectorizer(ngram_range=(2, 5), analyzer='word')
fit = cv2.fit_transform(content)
twoToFive = pd.DataFrame(fit.toarray(), columns=cv2.get_feature_names())
print(wordBYword)
print(twoToFive)

```

---

         also  always  as  asmr  be  beats  believe  communicate  computers  \
      0     0       0   0     0   1      0        1            0          2
      1     0       0   0     0   1      0        0            0          0
      2     1       1   1     1   0      1        0            1          0

         decoder  ...  this  to  transmit  using  via  video  watch  we  will  you
      0        0  ...     1   0         0      0    0      1      0   0     2    1
      1        0  ...     1   0         0      0    0      0      0   1     1    0
      2        1  ...     1   3         1      1    1      1      1   0     2    3

      [3 rows x 55 columns]
         also show  also show you  also show you how  also show you how to  \
      0          0              0                  0                     0
      1          0              0                  0                     0
      2          1              1                  1                     1

         always to  always to some  always to some nice  \
      0          0               0                    0
      1          0               0                    0
      2          1               1                    1

         always to some nice musical  as always  as always to  ...  \
      0                            0          0             0  ...
      1                            0          0             0  ...
      2                            1          1             1  ...

         you how to communicate  you how to communicate using  you know  \
      0                       0                             0         0
      1                       0                             0         0
      2                       1                             1         1

         you know the  you know the thing  you know the thing that  you watch  \
      0             0                   0                        0          0
      1             0                   0                        0          0
      2             1                   1                        1          1

         you watch mrbeast  you watch mrbeast or  you watch mrbeast or pewdiepie
      0                  0                     0                               0
      1                  0                     0                               0
      2                  1                     1                               1

      [3 rows x 278 columns]

---

You may have notice that this model tends to weigh popular words in these video descriptinos such as "also" or "you". This is where the **Tfidfvectorizer** comes in handy. Lets look at a similar example with TfidVectorizer instead.

```python
# %%

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
f = open(
    '..\CODE\INPUT\COUNTVEC_TEST.txt', 'r')
content = f.read()
content = str.split(content, '>')
cv = TfidfVectorizer()
fit = cv.fit_transform(content)
wordBYword = pd.DataFrame(fit.toarray(), columns=cv.get_feature_names())
print(wordBYword)

# %%

```

---

            also    always        as      asmr        be     beats   believe  \
      0  0.000000  0.000000  0.000000  0.000000  0.211054  0.000000  0.277511
      1  0.000000  0.000000  0.000000  0.000000  0.179873  0.000000  0.000000
      2  0.140933  0.140933  0.140933  0.140933  0.000000  0.140933  0.000000

         communicate  computers   decoder  ...      this        to  transmit  \
      0     0.000000   0.555021  0.000000  ...  0.163902  0.000000  0.000000
      1     0.000000   0.000000  0.000000  ...  0.139688  0.000000  0.000000
      2     0.140933   0.000000  0.140933  ...  0.083237  0.422798  0.140933

            using       via     video     watch        we      will       you
      0  0.000000  0.000000  0.211054  0.000000  0.000000  0.327805  0.211054
      1  0.000000  0.000000  0.000000  0.000000  0.236512  0.139688  0.000000
      2  0.140933  0.140933  0.107183  0.140933  0.000000  0.166474  0.321549

      [3 rows x 55 columns]

---

#### Image Featuers

Image proccessing is rather difficult as it has a low of data. An example is if you had a 256px by 256px image. Each of the pixels has a color value (usally 4 digits representing red, green, blue and alpha), and positon value. This would be very tedious luckally there is Scikit-image which can analize this images for us. Below is a short example of how to use this module.

```python
from skimage import data, color, feature
import skimage.data

```

### Picking a Model: What will you use?

### 3.3. Model Validation: Will this Model Work

### 3.4. Special Topic: Random Forests

There is a unique form of model called the **random forests** model. This model is is an _ensemble learning_ which uses simple dicision trees to calcualte its outcome.

A decision tree is a type of binary tree that uses a seires of decisions to select it out. This could be used to output your data in a unique manner. Below is an example of a simple decision tree to find the quadrant of the Cartesian plane a specific data point is in:

```mermaid
graph TB
   A[Is x > 0] -->|YES| B(Is y > 0)
   A -->|NO| C(Is y > 0)
   B -->|YES| D(ONE)
   B -->|NO| E(FOUR)
   C -->|YES| F(TWO)
   C -->|NO| G(THREE)
```

Firure 1: Decision tree example

Esentially a Random Forest creates this decision tree for us. This makes random forests a excelent unserpervised classification model.

You can build the dicision tree through a _DecisionTreeClassifier_ which is under the sklearn.tree section. But when fitting you must be careful to avoid <ins>Over-fitting</ins>. Over-fitting is when your model becomes "Attached" to your data and will look at the specific data points. Luckally there is a simple solution. Use multiple instances of your model fit to similar or the same data points and use all of them to make the final decision. The second method with Decision Tree models is to uses the ensemle method _bagging_ which quickly creates multiple parrael decsion trees of data. The group of random decision trees is where we get random forests. Luckally sklearn.ensemble include the RnadomForestClassifier estimator which easily helps your form a random forest rapidly.

In addtion to classification using a `RandomForestClassifier` you can use a `RnadomForestRegressor` for regression. This type of model is excelent at detecting oscilating functions and does not need much instructions on that front.

Below is an example of how to use a RandomForestClassifier:

---

CODE:

```python
# %% Setup
from sklearn.datasets import make_blobs
from sklearn.ensemble import RandomForestClassifier as RandForClassy
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()

# %% Getting Data

X, y = make_blobs(500, 2, centers=10, cluster_std=1, random_state=1892)
ax = plt.ax
plt.scatter(X[:, 0], X[:, 1], c=y, cmap='jet')
plt.savefig('..\images\RAND_FOREST-CLASS-DATA.jpg')
# %% Helper Function From Text


def visualize_classifier(model, X, y, ax=None, cmap='rainbow'):
    ax = ax or plt.gca()  # Set the Plot axis

    # Plot the training points
    ax.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=cmap,
               clim=(y.min(), y.max()), zorder=3)  # Create a scatter plot of the data
    ax.axis('tight')  # Set the axis Range to tight
    ax.axis('off')  # Turn Off Axis Desplay
    xlim = ax.get_xlim()  # Get The X/Y LIMITS
    ylim = ax.get_ylim()

    # fit the estimator
    model.fit(X, y)  # Fit the model
    xx, yy = np.meshgrid(np.linspace(*xlim, num=200),
                         np.linspace(*ylim, num=200))  # Make grid of datapoints
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(
        xx.shape)  # Use model to predict data

    # Create a color plot with the results
    n_classes = len(np.unique(y))
    contours = ax.contourf(xx, yy, Z, alpha=0.3,
                           levels=np.arange(n_classes + 1) - 0.5,
                           cmap=cmap, clim=(y.min(), y.max()),
                           zorder=1)

    ax.set(xlim=xlim, ylim=ylim)


# %% Setting UP and Running Module

model = RandForClassy(n_estimators=200)
visualize_classifier(model, X, y, cmap='seismic')
plt.savefig('..\images\RAND_FOREST-CLASS-MODEL_200.jpg')
model = RandForClassy(n_estimators=500)
visualize_classifier(model, X, y, cmap='rainbow')
plt.savefig('..\images\RNAD_FOREST-CLASS-MODEL_500.jpg')
# %%
```

The output looks like this

![DATA](images/RAND_FOREST-CLASS-DATA.jpg)
![MODEL with 200 DTres](images/RAND_FOREST-CLASS-MODEL_200.jpg)
![Model with 500 DTrees](images/RNAD_FOREST-CLASS-MODEL_500.jpg)

If you zoom into the image files you might notice that these models are albe to show a reagon to show the teal category that mixes within the red, yellow and blue categories. Sadly due to the image quality it is hard to tell how well the 500 group and 200 group system worked.

In the Electronic Warfare (EW) industy analyzing signals through noise is neccesary. Therefore a unquie cabability for Cognitive EW (CEW) is achived via Rand Forest Regression.

```python
### CEW EXAMPLE ###
# %% Setup
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from scipy.signal import sawtooth
from sklearn.datasets import make_blobs
from sklearn.ensemble import RandomForestRegressor as RandForRegy

sns.set()
rng = np.random.RandomState(420)
x = np.linspace(0, 12, 200)
# SIGNAL FUNCTION


def signal(x, noise_mult=1):
    base = np.sin(2*np.pi*x)
    saw = sawtooth(3*np.pi*x)
    noise_a = 0.5 * (np.random.randint(-90, 90, len(x))/100)
    noise_b = 0.2 * np.random.randint(0, 1) * np.cos(3*sawtooth(x)+base)
    noise = (noise_a + noise_b)
    return base + saw + noise*noise_mult


y = signal(x)


# %% SETTING UP/RUNNING OUT MODEL
model_200 = RandForRegy(200)
model_500 = RandForRegy(500)

model_200.fit(x[:, None], y)
model_500.fit(x[:, None], y)

y_fit_200 = model_200.predict(x[:, None])
y_fit_500 = model_500.predict(x[:, None])
# %% FILL OUT


plt.plot(x, y_fit_200, '--', linewidth=2, label='200_fit')
plt.plot(x, y_fit_500, '-.', linewidth=2, label='500_fit')
plt.plot(x, signal(x, noise_mult=0), alpha=0.4, c='red', label='True')
plt.errorbar(x, y, 0.5, fmt='o', alpha=0.2, label='Noisy')
plt.legend(loc='best')
plt.savefig('..\images\RAND_FOREST-REG-ALL.png')
# %%
```

![Data Output](images/RAND_FOREST-REG-ALL.jpg)

You may notice that despite the noise the data actually does get the underlying siganl despite the noise.

## 4. Project: Face Detection Pipeline

### 4.1. Output

### 4.2. Code
