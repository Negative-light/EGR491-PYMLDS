# Project 5: Making an AI

## 1. Table of Display

|                                Topic | Method         |
| -----------------------------------: | :------------- |
|                           What is ML | Summary        |
|                Intro to Scikit-Learn | Summary w/Code |
| Hyperparameters and Model Validation | ?              |
|                  Feature Engineering | ?              |
|            DTrees and Random Forests | ?              |
|              Face Detection Pipeline | Final Project  |

## 2. Table Of Contents

1. [1. Table of Display](#1-table-of-display)
2. [2. Table Of Contents](#2-table-of-contents)
3. [3. Background](#3-background)
   1. [3.1. What is Machine Learning?](#31-what-is-machine-learning)
      1. [3.1.1. Supervised learning](#311-supervised-learning)
         1. [3.1.1.1. Classification](#3111-classification)
         2. [3.1.1.2. Regression](#3112-regression)
      2. [3.1.2. Unspervised Learning](#312-unspervised-learning)
         1. [3.1.2.1. Clustering](#3121-clustering)
         2. [3.1.2.2. Dimensionality Reduction](#3122-dimensionality-reduction)
   2. [3.2. Scikit-Learn](#32-scikit-learn)
      1. [3.2.1. Data Represtientation](#321-data-represtientation)
      2. [3.2.2. Essentials of Scikit](#322-essentials-of-scikit)
         1. [3.2.2.1. Using Scikit-Learn](#3221-using-scikit-learn)
   3. [3.3. Model Validation](#33-model-validation)
   4. [Model Selection](#model-selection)
   5. [Hyperparmeters](#hyperparmeters)
      1. [Holdout Sets](#holdout-sets)
   6. [3.5. Feature Engineering](#35-feature-engineering)
   7. [3.6. Special Topic](#36-special-topic)
4. [4. Project: Face Detection Pipeline](#4-project-face-detection-pipeline)
   1. [4.1. Output](#41-output)
   2. [4.2. Code](#42-code)

## 3. Background

### 3.1. What is Machine Learning?

Machine Learning (ML) is the art of designing mathmatical models of data which can then be taught via tunable parameters. Because these algorithms are designed to asist in understanding data, there can be some debaste of whether ML could be considered a branch of Aritifical Intelegence (AI) anymore.

Because ML works with big data which vary greatly in both size, complexity there must also be various methods of analizing this data. The two general categories of ML are **Supervised** and **Unsupervised** learning. We will explorw these methods futher in this paper.

#### 3.1.1. Supervised learning

Supervised learning takes data and labels associated with the data to model their relationship. This ML model is used to apply labels to novel data. Supervised learning is commonly subdivided into **classification** and **regression**.

##### 3.1.1.1. Classification

Classification models use descrite categories such as a status. This type of model may be used to identify objects in an image or seek for when a part might be damaged within a piece of machinery.

This model will require the devleoper to fist make a labeled dataset. Then you must design model inputs and the general assumptions your can provide. After this you can provide a set of model paramters which can be adjusted by the model during the training stage. The end requlst is that when introduced to novel data the classical model will provide a predictive label.

##### 3.1.1.2. Regression

Regression models place their categories in a continuous spectrums. This can be used to seek the pobablity of damage to an piece of machinery or seek more complex relationships such as genetics to regions.

Regression models could be treadted as the oposite of [Demensionality Reduction](#3122-dimensionality-reduction) models. They will extract a new unknown relationship and create a new dimesion of labels.

#### 3.1.2. Unspervised Learning

Unsuprvised learning results from not having a labeled dataset. This type of learning is used to find relationships within a data set. Unspervised Learning can be further subdivided into **Clustering** and **Demensitionaly Reduction** models.

##### 3.1.2.1. Clustering

Clustering models act simlarly to classification models exceept they are seeking the distinct groups with the dataset. This can be used to seekout groups within massive datasets that humans may never see.

One method to do clustersing is through the _k_-means fits model. This method finds the center of data clusters, it must be profivided with a value _k_ or this value may be tunable. This model seeks the postion of centers that has the minimum distance between all points in the dataset.

##### 3.1.2.2. Dimensionality Reduction

Demensionality Reduction as its name suggests is designed to simpfly a data set into the smalled scturcure possible. This type of model could be used to seek important parameters to watch in larger systems which may take years for a human to analyze. This method allows us to infer new strutures that may not be labels (or may not exist).

A demnstionality reduction model typcially will remove one or more of the layers of data. This could be used to review a large dataset and place the data neatly into graphs that huamns and easily understand, interpret and apply.

### 3.2. Scikit-Learn

While developing a ML algrithem from scratch is possible it could be cumbersom and likly has already been done. Thus to prevent reinventing the wheel, you can use the modeule **Scikit-Learn**. Scikit-Learn contains many of the popular algorithems in one consitent API.

#### 3.2.1. Data Represtientation

Scikit-Learn descibes data with a _features matrix_ this mastrix can be stored in a NumPy array or Pandas DataFrame. The rows of the features matrix are often called sameles and the columns are considered features. Thus the features matrix is considered to have the shape `[n_samples, n_features]`.

The features matrix is then coupled with a _label array_. This array is typically is a Numpy array or Pandas Series of n_samples, but it can be two-dimesional with the shapre [n_smaple, n_targets]. The label array could be considered the dependent variable the model is attempting to predict.

#### 3.2.2. Essentials of Scikit

Scikit is designed to a _consitant_ API used for _inspection_ of data. It also procvides a simplistic _composition_ through the simplification of complex ML algorithms into their foundational parts. It also uses common data structures and provides excellent user-oriented defaults.

This results in the following workflow for algorithm design.

```Mermaid
graph TB
   A(Choose Class Model) --> B(Choose Hyperpamters)
   B --> C[Arrange Data into Features Matrix and Label Matrix]
   C --> D(Fit Model To Data)
   D --> E[Apply Model To new data]
```

##### 3.2.2.1. Using Scikit-Learn

Each model class is stored as a class within Scikit-Learn and are typically imported as a single object. For instance to import the _LinearRegression_ modle you would use

```python
## ALL THE IMPORTS
# IMPORTS
import matplotlib.pyplot as plt # PLOTING
import numpy as np # NUMPY
import pandas as pd # PANDAS
from sklearn.datasets import load_iris # Dataset
from sklearn.metrics import accuracy_score # Metrics for analysis
# Model Validation Tools
from sklearn.model_selection import train_test_split

# Models
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neighbors import RadiusNeighborsClassifier
# LOAD IRIS DATA
iris = load_iris()
X = iris.data
y = iris.target
```

### 3.3. Model Validation

After selecting a model and selecting its hyperparameters you must then _validate_ the model. The two methods of model validation we will discus are **holdout sets** and **cross validation**. Once you have validated a model you must then select the best model by balancing the bias and variance of that model for the data you are using.

### Model Selection

### Hyperparmeters

#### Holdout Sets

### 3.5. Feature Engineering

### 3.6. Special Topic

## 4. Project: Face Detection Pipeline

### 4.1. Output

### 4.2. Code
